{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考\n",
    "- https://github.com/gaussic/text-classification mxnet(gluon)/pytorch实现\n",
    "- https://mxnet.incubator.apache.org/tutorials/nlp/cnn.html mxnet(sym scratch) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test split: 9662/1000\n",
      "train shape: (9662, 56)\n",
      "test shape: (1000, 56)\n",
      "vocab_size 18766\n",
      "sentence max words 56\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "try:\n",
    "    # For Python 3.0 and later\n",
    "    from urllib.request import urlopen\n",
    "except ImportError:\n",
    "    # Fall back to Python 2's urllib2\n",
    "    from urllib2 import urlopen\n",
    "    \n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    return string.strip().lower()\n",
    "\n",
    "def download_sentences(url):\n",
    "    \"\"\"\n",
    "    Download sentences from specified URL. \n",
    "    \n",
    "    Strip trailing newline, convert to Unicode.\n",
    "    \"\"\"\n",
    "    \n",
    "    remote_file = urlopen(url)\n",
    "    return [line.decode('Latin1').strip() for line in remote_file.readlines()]\n",
    "    \n",
    "def load_data_and_labels():\n",
    "    \"\"\"\n",
    "    Loads polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    positive_examples = download_sentences('https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.pos')\n",
    "    negative_examples = download_sentences('https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.neg')\n",
    "    \n",
    "    # Tokenize\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_str(sent).split(\" \") for sent in x_text]\n",
    "\n",
    "    # Generate labels\n",
    "    positive_labels = [1 for _ in positive_examples]\n",
    "    negative_labels = [0 for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return x_text, y\n",
    "\n",
    "\n",
    "def pad_sentences(sentences, padding_word=\"\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to be the length of the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "        \n",
    "    return padded_sentences\n",
    "\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from token to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    \n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    \n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    \n",
    "    return vocabulary, vocabulary_inv\n",
    "\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentences and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = np.array([\n",
    "            [vocabulary[word] for word in sentence]\n",
    "            for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\"\"\"\n",
    "Loads and preprocesses data for the MR dataset.\n",
    "Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "\"\"\"\n",
    "# Load and preprocess data\n",
    "sentences, labels = load_data_and_labels()\n",
    "sentences_padded = pad_sentences(sentences)\n",
    "vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# split train/test set\n",
    "# there are a total of 10662 labeled examples to train on\n",
    "x_train, x_test = x_shuffled[:-1000], x_shuffled[-1000:]\n",
    "y_train, y_test = y_shuffled[:-1000], y_shuffled[-1000:]\n",
    "\n",
    "sentence_size = x_train.shape[1]\n",
    "\n",
    "print('Train/Test split: %d/%d' % (len(y_train), len(y_test)))\n",
    "print('train shape:', x_train.shape)\n",
    "print('test shape:', x_test.shape)\n",
    "print('vocab_size', vocab_size)\n",
    "print('sentence max words', sentence_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gen train test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/optimizer.py:136: UserWarning: WARNING: New optimizer mxnet.optimizer.NAG is overriding existing optimizer mxnet.optimizer.NAG\n",
      "  Optimizer.opt_registry[name].__name__))\n"
     ]
    }
   ],
   "source": [
    "from mxnet import gluon\n",
    "batch_size = 50\n",
    "dataset_train = gluon.data.ArrayDataset(x_train.astype('float32'), y_train.astype('float32'))\n",
    "train_data = gluon.data.DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "dataset_test = gluon.data.ArrayDataset(x_test.astype('float32'), y_test.astype('float32'))\n",
    "test_data = gluon.data.DataLoader(dataset_test, batch_size, shuffle=True)\n",
    "for data, label in test_data:\n",
    "    print(data, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "import utilstmp\n",
    "import time\n",
    "\n",
    "def net_structure(num_hidden, num_outputs, ctx, num_embed):\n",
    "    net = gluon.nn.Sequential()\n",
    "    num_embed = num_embed # dimensions to embed words into\n",
    "    filter_size = 3\n",
    "    \n",
    "    with net.name_scope():\n",
    "        net.add(\n",
    "            nn.Embedding(vocab_size, num_embed),\n",
    "            nn.Conv1D(num_embed, 3),\n",
    "            nn.GlobalMaxPool1D(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Dense(num_outputs)\n",
    "        )\n",
    "        print(net)\n",
    "        print(net.collect_params())\n",
    "    net.initialize(ctx=ctx)\n",
    "    return net\n",
    "\n",
    "def mlp(optimizer='sgd', num_outputs=2, num_hidden=256, weight_scale=.01, learning_rate=0.0005, \n",
    "        num_epoch=10, batch_size=50, num_embed=300):\n",
    "    batch_size = batch_size\n",
    "\n",
    "    ctx = utilstmp.try_gpu(device_id=1)\n",
    "    net = net_structure(num_hidden=num_hidden, num_outputs=num_outputs, ctx=ctx, num_embed=num_embed)\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), optimizer, {'learning_rate': learning_rate})\n",
    "    utilstmp.train(train_data, test_data, net, loss, trainer, ctx, num_epochs=num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Embedding(18766 -> 128, float32)\n",
      "  (1): Conv1D(None -> 128, kernel_size=(3,), stride=(1,))\n",
      "  (2): GlobalMaxPool1D(size=(1,), stride=(1,), padding=(0,), ceil_mode=True)\n",
      "  (3): Dropout(p = 0.5)\n",
      "  (4): Dense(None -> 2, linear)\n",
      ")\n",
      "sequential1_ (\n",
      "  Parameter sequential1_embedding0_weight (shape=(18766, 128), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_conv0_weight (shape=(128, 0, 3), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_conv0_bias (shape=(128,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_dense0_weight (shape=(2, 0), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_dense0_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
      ")\n",
      "Start training on  gpu(1)\n",
      "Epoch 0. Loss: 0.694, Train acc 0.51, Test acc 0.52, Time 1.3 sec\n",
      "Epoch 1. Loss: 0.529, Train acc 0.73, Test acc 0.75, Time 1.1 sec\n",
      "Epoch 2. Loss: 0.195, Train acc 0.92, Test acc 0.75, Time 1.1 sec\n",
      "Epoch 3. Loss: 0.037, Train acc 0.99, Test acc 0.77, Time 1.1 sec\n",
      "Epoch 4. Loss: 0.006, Train acc 1.00, Test acc 0.77, Time 1.1 sec\n",
      "Epoch 5. Loss: 0.001, Train acc 1.00, Test acc 0.77, Time 1.1 sec\n",
      "Epoch 6. Loss: 0.001, Train acc 1.00, Test acc 0.77, Time 1.1 sec\n",
      "Epoch 7. Loss: 0.001, Train acc 1.00, Test acc 0.77, Time 1.1 sec\n",
      "Epoch 8. Loss: 0.000, Train acc 1.00, Test acc 0.77, Time 1.1 sec\n",
      "Epoch 9. Loss: 0.000, Train acc 1.00, Test acc 0.77, Time 1.1 sec\n"
     ]
    }
   ],
   "source": [
    "mlp(learning_rate=0.003, num_epoch=10, batch_size=5, optimizer='adam', num_embed=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
