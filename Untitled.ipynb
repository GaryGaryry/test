{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考\n",
    "- https://github.com/gaussic/text-classification mxnet(gluon)/pytorch实现\n",
    "- https://mxnet.incubator.apache.org/tutorials/nlp/cnn.html mxnet(sym scratch) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test split: 9662/1000\n",
      "train shape: (9662, 56)\n",
      "test shape: (1000, 56)\n",
      "vocab_size 18766\n",
      "sentence max words 56\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "try:\n",
    "    # For Python 3.0 and later\n",
    "    from urllib.request import urlopen\n",
    "except ImportError:\n",
    "    # Fall back to Python 2's urllib2\n",
    "    from urllib2 import urlopen\n",
    "    \n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    return string.strip().lower()\n",
    "\n",
    "def download_sentences(url):\n",
    "    \"\"\"\n",
    "    Download sentences from specified URL. \n",
    "    \n",
    "    Strip trailing newline, convert to Unicode.\n",
    "    \"\"\"\n",
    "    \n",
    "    remote_file = urlopen(url)\n",
    "    return [line.decode('Latin1').strip() for line in remote_file.readlines()]\n",
    "    \n",
    "def load_data_and_labels():\n",
    "    \"\"\"\n",
    "    Loads polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    positive_examples = download_sentences('https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.pos')\n",
    "    negative_examples = download_sentences('https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.neg')\n",
    "    \n",
    "    # Tokenize\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_str(sent).split(\" \") for sent in x_text]\n",
    "\n",
    "    # Generate labels\n",
    "    positive_labels = [1 for _ in positive_examples]\n",
    "    negative_labels = [0 for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return x_text, y\n",
    "\n",
    "\n",
    "def pad_sentences(sentences, padding_word=\"\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to be the length of the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "        \n",
    "    return padded_sentences\n",
    "\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from token to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    \n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    \n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    \n",
    "    return vocabulary, vocabulary_inv\n",
    "\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentences and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = np.array([\n",
    "            [vocabulary[word] for word in sentence]\n",
    "            for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\"\"\"\n",
    "Loads and preprocesses data for the MR dataset.\n",
    "Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "\"\"\"\n",
    "# Load and preprocess data\n",
    "sentences, labels = load_data_and_labels()\n",
    "sentences_padded = pad_sentences(sentences)\n",
    "vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# split train/test set\n",
    "# there are a total of 10662 labeled examples to train on\n",
    "x_train, x_test = x_shuffled[:-1000], x_shuffled[-1000:]\n",
    "y_train, y_test = y_shuffled[:-1000], y_shuffled[-1000:]\n",
    "\n",
    "sentence_size = x_train.shape[1]\n",
    "\n",
    "print('Train/Test split: %d/%d' % (len(y_train), len(y_test)))\n",
    "print('train shape:', x_train.shape)\n",
    "print('test shape:', x_test.shape)\n",
    "print('vocab_size', vocab_size)\n",
    "print('sentence max words', sentence_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gen train test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/optimizer.py:136: UserWarning: WARNING: New optimizer mxnet.optimizer.NAG is overriding existing optimizer mxnet.optimizer.NAG\n",
      "  Optimizer.opt_registry[name].__name__))\n"
     ]
    }
   ],
   "source": [
    "from mxnet import gluon\n",
    "batch_size = 50\n",
    "dataset_train = gluon.data.ArrayDataset(x_train.astype('float32'), y_train.astype('float32'))\n",
    "train_data = gluon.data.DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "dataset_test = gluon.data.ArrayDataset(x_test.astype('float32'), y_test.astype('float32'))\n",
    "test_data = gluon.data.DataLoader(dataset_test, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[  1.70000000e+01   2.70000000e+01   7.00000000e+00 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  2.75600000e+03   8.00000000e+00   1.38000000e+02 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  1.31430000e+04   3.40000000e+01   1.00000000e+00 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " ..., \n",
      " [  1.00000000e+00   1.40000000e+01   8.30000000e+01 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  6.00000000e+00   5.68700000e+03   1.91000000e+02 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  4.50000000e+01   1.41000000e+02   1.88000000e+02 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n",
      "<NDArray 50x56 @cpu(0)> \n",
      "[ 0.  1.  1.  1.  0.  0.  1.  0.  0.  1.  1.  1.  0.  0.  0.  1.  1.  0.\n",
      "  1.  1.  1.  0.  1.  1.  0.  1.  0.  1.  0.  1.  1.  1.  0.  1.  1.  1.\n",
      "  0.  0.  0.  1.  1.  1.  1.  1.  1.  0.  1.  0.  0.  0.]\n",
      "<NDArray 50 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "for data, label in test_data:\n",
    "    print(data, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "batch_size = 50\n",
    "dataset_train = gluon.data.ArrayDataset(x_train.reshape(x_train.shape[0], 1, x_train.shape[1]).astype('float32'), y_train.astype('float32'))\n",
    "train_data = gluon.data.DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "dataset_test = gluon.data.ArrayDataset(x_test.reshape(x_test.shape[0], 1, x_test.shape[1]).astype('float32'), y_test.astype('float32'))\n",
    "test_data = gluon.data.DataLoader(dataset_test, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = batch_size\n",
    "#     train_data, test_data = utils.load_data_fashion_mnist(batch_size)\n",
    "ctx = utilstmp.try_gpu()\n",
    "#     net = net_structure(num_hidden=num_hidden, num_outputs=num_outputs, ctx=ctx)\n",
    "net = gluon.nn.Sequential()\n",
    "num_embed = 300 # dimensions to embed words into\n",
    "filter_size = 3\n",
    "\n",
    "with net.name_scope():\n",
    "#     net.add(\n",
    "#         nn.Embedding(vocab_size, num_embed),\n",
    "#         nn.Conv1D(num_embed, 3),\n",
    "#         nn.GlobalMaxPool1D(),\n",
    "#         nn.Dropout(0.5),\n",
    "#         nn.Dense(2)\n",
    "#     )\n",
    "    net.add(gluon.nn.Embedding(input_dim=vocab_size, output_dim=num_embed))\n",
    "    # 输入输出数据格式是 batch x channel x height x width\n",
    "    net.add(gluon.nn.Conv1D(channels=1, kernel_size=filter_size, activation='relu'))\n",
    "    net.add(gluon.nn.GlobalMaxPool1D())\n",
    "    net.add(gluon.nn.Dropout(0.5))\n",
    "    net.add(gluon.nn.Dense(2, activation=\"relu\"))\n",
    "net.initialize(ctx=ctx)\n",
    "#     softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = utilstmp.try_gpu()\n",
    "config = TCNNConfig()\n",
    "config.vocab_size = vocab_size\n",
    "V = config.vocab_size\n",
    "E = config.embedding_dim\n",
    "Nf = config.num_filters\n",
    "Ks = config.kernel_sizes\n",
    "C = config.num_classes\n",
    "Dr = config.dropout_prob\n",
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(\n",
    "        nn.Embedding(V, E),\n",
    "        nn.Conv1D(Nf, Ks[0]),\n",
    "        nn.GlobalMaxPool1D(),\n",
    "        nn.Dropout(Dr),\n",
    "        nn.Dense(C)\n",
    "    )\n",
    "net.initialize(ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training on  gpu(0)\n",
      "Epoch 0. Loss: 0.694, Train acc 0.50, Test acc 0.48, Time 1.3 sec\n",
      "Epoch 1. Loss: 0.683, Train acc 0.57, Test acc 0.60, Time 1.1 sec\n",
      "Epoch 2. Loss: 0.518, Train acc 0.76, Test acc 0.73, Time 1.1 sec\n",
      "Epoch 3. Loss: 0.256, Train acc 0.91, Test acc 0.74, Time 1.1 sec\n",
      "Epoch 4. Loss: 0.096, Train acc 0.97, Test acc 0.73, Time 1.1 sec\n",
      "Epoch 5. Loss: 0.032, Train acc 0.99, Test acc 0.73, Time 1.1 sec\n",
      "Epoch 6. Loss: 0.013, Train acc 1.00, Test acc 0.73, Time 1.1 sec\n",
      "Epoch 7. Loss: 0.007, Train acc 1.00, Test acc 0.73, Time 1.1 sec\n",
      "Epoch 8. Loss: 0.004, Train acc 1.00, Test acc 0.73, Time 1.1 sec\n",
      "Epoch 9. Loss: 0.003, Train acc 1.00, Test acc 0.73, Time 1.1 sec\n"
     ]
    }
   ],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': config.learning_rate})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "utilstmp.train(train_data, test_data, net, loss, trainer, ctx, num_epochs=config.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training on  gpu(0)\n",
      "Epoch 0. Loss: 0.694, Train acc 0.49, Test acc 0.51, Time 1.1 sec\n",
      "Epoch 1. Loss: 0.693, Train acc 0.50, Test acc 0.50, Time 1.0 sec\n",
      "Epoch 2. Loss: 0.693, Train acc 0.50, Test acc 0.50, Time 1.0 sec\n",
      "Epoch 3. Loss: 0.693, Train acc 0.50, Test acc 0.49, Time 1.0 sec\n",
      "Epoch 4. Loss: 0.693, Train acc 0.50, Test acc 0.50, Time 1.1 sec\n"
     ]
    }
   ],
   "source": [
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': config.learning_rate})\n",
    "#     import pdb\n",
    "#     pdb.set_trace() \n",
    "utilstmp.train(train_data, test_data, net, loss, trainer, ctx, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training on  gpu(0)\n",
      "Epoch 0. Loss: 0.693, Train acc 0.49, Test acc 0.52, Time 1.9 sec\n",
      "Epoch 1. Loss: 0.693, Train acc 0.50, Test acc 0.52, Time 1.2 sec\n",
      "Epoch 2. Loss: 0.693, Train acc 0.51, Test acc 0.48, Time 1.2 sec\n",
      "Epoch 3. Loss: 0.693, Train acc 0.50, Test acc 0.48, Time 1.2 sec\n",
      "Epoch 4. Loss: 0.692, Train acc 0.50, Test acc 0.48, Time 1.2 sec\n",
      "Epoch 5. Loss: 0.690, Train acc 0.49, Test acc 0.51, Time 1.2 sec\n",
      "Epoch 6. Loss: 0.681, Train acc 0.51, Test acc 0.56, Time 1.2 sec\n",
      "Epoch 7. Loss: 0.667, Train acc 0.55, Test acc 0.58, Time 1.2 sec\n",
      "Epoch 8. Loss: 0.651, Train acc 0.57, Test acc 0.58, Time 1.2 sec\n",
      "Epoch 9. Loss: 0.631, Train acc 0.60, Test acc 0.60, Time 1.2 sec\n",
      "Epoch 10. Loss: 0.623, Train acc 0.62, Test acc 0.62, Time 1.2 sec\n",
      "Epoch 11. Loss: 0.604, Train acc 0.65, Test acc 0.63, Time 1.2 sec\n",
      "Epoch 12. Loss: 0.592, Train acc 0.66, Test acc 0.63, Time 1.2 sec\n",
      "Epoch 13. Loss: 0.582, Train acc 0.68, Test acc 0.65, Time 1.2 sec\n",
      "Epoch 14. Loss: 0.576, Train acc 0.68, Test acc 0.65, Time 1.2 sec\n",
      "Epoch 15. Loss: 0.570, Train acc 0.69, Test acc 0.66, Time 1.2 sec\n",
      "Epoch 16. Loss: 0.559, Train acc 0.70, Test acc 0.65, Time 1.2 sec\n",
      "Epoch 17. Loss: 0.555, Train acc 0.71, Test acc 0.66, Time 1.2 sec\n",
      "Epoch 18. Loss: 0.545, Train acc 0.72, Test acc 0.66, Time 1.2 sec\n",
      "Epoch 19. Loss: 0.551, Train acc 0.71, Test acc 0.66, Time 1.2 sec\n",
      "Epoch 20. Loss: 0.540, Train acc 0.73, Test acc 0.66, Time 1.2 sec\n",
      "Epoch 21. Loss: 0.536, Train acc 0.73, Test acc 0.66, Time 1.2 sec\n",
      "Epoch 22. Loss: 0.544, Train acc 0.72, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 23. Loss: 0.532, Train acc 0.73, Test acc 0.68, Time 1.2 sec\n",
      "Epoch 24. Loss: 0.537, Train acc 0.73, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 25. Loss: 0.535, Train acc 0.73, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 26. Loss: 0.535, Train acc 0.73, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 27. Loss: 0.535, Train acc 0.73, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 28. Loss: 0.526, Train acc 0.74, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 29. Loss: 0.527, Train acc 0.74, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 30. Loss: 0.524, Train acc 0.74, Test acc 0.68, Time 1.2 sec\n",
      "Epoch 31. Loss: 0.527, Train acc 0.74, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 32. Loss: 0.526, Train acc 0.74, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 33. Loss: 0.524, Train acc 0.74, Test acc 0.68, Time 1.2 sec\n",
      "Epoch 34. Loss: 0.519, Train acc 0.75, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 35. Loss: 0.524, Train acc 0.74, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 36. Loss: 0.523, Train acc 0.74, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 37. Loss: 0.523, Train acc 0.74, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 38. Loss: 0.524, Train acc 0.74, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 39. Loss: 0.523, Train acc 0.74, Test acc 0.66, Time 1.2 sec\n",
      "Epoch 40. Loss: 0.518, Train acc 0.75, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 41. Loss: 0.520, Train acc 0.75, Test acc 0.66, Time 1.2 sec\n",
      "Epoch 42. Loss: 0.524, Train acc 0.74, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 43. Loss: 0.524, Train acc 0.74, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 44. Loss: 0.525, Train acc 0.74, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 45. Loss: 0.524, Train acc 0.74, Test acc 0.68, Time 1.2 sec\n",
      "Epoch 46. Loss: 0.524, Train acc 0.74, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 47. Loss: 0.521, Train acc 0.75, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 48. Loss: 0.520, Train acc 0.75, Test acc 0.67, Time 1.2 sec\n",
      "Epoch 49. Loss: 0.523, Train acc 0.74, Test acc 0.67, Time 1.2 sec\n"
     ]
    }
   ],
   "source": [
    "# %pdb on\n",
    "mlp(num_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "class TCNNConfig(object):\n",
    "    \"\"\"\n",
    "    CNN parameters\n",
    "    \"\"\"\n",
    "    embedding_dim = 128  # embedding vector size\n",
    "    seq_length = 50  # maximum length of sequence\n",
    "    vocab_size = 8000  # most common words\n",
    "\n",
    "    num_filters = 100  # number of the convolution filters (feature maps)\n",
    "    kernel_sizes = [3, 4, 5]  # three kinds of kernels (windows)\n",
    "\n",
    "    dropout_prob = 0.5  # dropout rate\n",
    "    learning_rate = 1e-3  # learning rate\n",
    "    batch_size = 50  # batch size for training\n",
    "    num_epochs = 10  # total number of epochs\n",
    "\n",
    "    num_classes = 2  # number of classes\n",
    "\n",
    "    test_split = 0.1  # percentage of test data\n",
    "\n",
    "\n",
    "class Conv_Max_Pooling(nn.Block):\n",
    "    \"\"\"\n",
    "    Integration of Conv1D and GlobalMaxPool1D layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, kernel_size, **kwargs):\n",
    "        super(Conv_Max_Pooling, self).__init__(**kwargs)\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.conv = nn.Conv1D(channels, kernel_size)\n",
    "            self.pooling = nn.GlobalMaxPool1D()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pooling(self.conv(x))\n",
    "        return nd.relu(output).flatten()\n",
    "\n",
    "\n",
    "class TextCNN(nn.Block):\n",
    "    \"\"\"\n",
    "    CNN text classification model, based on the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super(TextCNN, self).__init__(**kwargs)\n",
    "\n",
    "        V = config.vocab_size\n",
    "        E = config.embedding_dim\n",
    "        Nf = config.num_filters\n",
    "        Ks = config.kernel_sizes\n",
    "        C = config.num_classes\n",
    "        Dr = config.dropout_prob\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(V, E)  # embedding layer\n",
    "\n",
    "            # three different convolutional layers\n",
    "            self.conv1 = Conv_Max_Pooling(Nf, Ks[0])\n",
    "            self.conv2 = Conv_Max_Pooling(Nf, Ks[1])\n",
    "            self.conv3 = Conv_Max_Pooling(Nf, Ks[2])\n",
    "            self.dropout = nn.Dropout(Dr)  # a dropout layer\n",
    "            self.fc1 = nn.Dense(C)  # a dense layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).transpose((0, 2, 1))  # Conv1D takes in NCW as input\n",
    "        o1, o2, o3 = self.conv1(x), self.conv2(x), self.conv3(x)\n",
    "        outputs = self.fc1(self.dropout(nd.concat(o1, o2, o3)))\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "\n",
    "ctx = utilstmp.try_gpu()\n",
    "config = TCNNConfig()\n",
    "config.vocab_size = vocab_size\n",
    "V = config.vocab_size\n",
    "E = config.embedding_dim\n",
    "Nf = config.num_filters\n",
    "Ks = config.kernel_sizes\n",
    "C = config.num_classes\n",
    "Dr = config.dropout_prob\n",
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(\n",
    "        nn.Embedding(V, E),\n",
    "        nn.Conv1D(Nf, Ks[0]),\n",
    "        nn.GlobalMaxPool1D(),\n",
    "        nn.Dropout(Dr),\n",
    "        nn.Dense(C)\n",
    "    )\n",
    "net.initialize(ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training on  gpu(0)\n",
      "Epoch 0. Loss: 0.694, Train acc 0.50, Test acc 0.52, Time 1.9 sec\n",
      "Epoch 1. Loss: 0.686, Train acc 0.56, Test acc 0.52, Time 1.1 sec\n",
      "Epoch 2. Loss: 0.564, Train acc 0.72, Test acc 0.73, Time 1.1 sec\n",
      "Epoch 3. Loss: 0.301, Train acc 0.88, Test acc 0.75, Time 1.1 sec\n",
      "Epoch 4. Loss: 0.109, Train acc 0.97, Test acc 0.76, Time 1.1 sec\n",
      "Epoch 5. Loss: 0.032, Train acc 0.99, Test acc 0.75, Time 1.1 sec\n",
      "Epoch 6. Loss: 0.011, Train acc 1.00, Test acc 0.75, Time 1.1 sec\n",
      "Epoch 7. Loss: 0.006, Train acc 1.00, Test acc 0.76, Time 1.1 sec\n",
      "Epoch 8. Loss: 0.003, Train acc 1.00, Test acc 0.76, Time 1.1 sec\n",
      "Epoch 9. Loss: 0.002, Train acc 1.00, Test acc 0.75, Time 1.1 sec\n"
     ]
    }
   ],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': config.learning_rate})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "utilstmp.train(train_data, test_data, net, loss, trainer, ctx, num_epochs=config.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "import utilstmp\n",
    "import time\n",
    "\n",
    "def net_structure(num_hidden, num_outputs, ctx):\n",
    "    net = gluon.nn.Sequential()\n",
    "    num_embed = 300 # dimensions to embed words into\n",
    "    filter_size = 3\n",
    "    \n",
    "    with net.name_scope():\n",
    "#         net.add(\n",
    "#             nn.Embedding(vocab_size, num_embed),\n",
    "#             nn.Conv1D(num_embed, 3),\n",
    "#             nn.GlobalMaxPool1D(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Dense(num_outputs)\n",
    "#         )\n",
    "#         net.add(gluon.nn.Flatten())\n",
    "        net.add(gluon.nn.Embedding(input_dim=vocab_size, output_dim=num_embed))\n",
    "        # 输入输出数据格式是 batch x channel x height x width\n",
    "        net.add(gluon.nn.Conv1D(channels=1, kernel_size=filter_size, activation='relu'))\n",
    "        net.add(gluon.nn.GlobalMaxPool1D())\n",
    "        net.add(gluon.nn.Dropout(0.5))\n",
    "        net.add(gluon.nn.Dense(num_outputs, activation=\"relu\"))\n",
    "#         net.add(gluon.nn.softmax_cross_entropy)\n",
    "        print(net)\n",
    "        print(net.collect_params())\n",
    "    \n",
    "    net.initialize(ctx=ctx)\n",
    "    return net\n",
    "\n",
    "def mlp(optimizer='sgd', num_outputs=2, num_hidden=256, weight_scale=.01, learning_rate=0.0005, \n",
    "        num_epoch=50, batch_size=50):\n",
    "    batch_size = batch_size\n",
    "#     train_data, test_data = utils.load_data_fashion_mnist(batch_size)\n",
    "    ctx = utilstmp.try_gpu()\n",
    "#     net = net_structure(num_hidden=num_hidden, num_outputs=num_outputs, ctx=ctx)\n",
    "#     softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': learning_rate})\n",
    "#     import pdb\n",
    "#     pdb.set_trace() \n",
    "    utilstmp.train(train_data, test_data, net, loss, trainer, ctx, num_epochs=num_epoch)\n",
    "#     for epoch in range(num_epoch):\n",
    "#         tic = time.time()\n",
    "#         train_loss = 0.\n",
    "#         train_acc = 0.\n",
    "#         for data, label in train_data:\n",
    "#             with autograd.record():\n",
    "#                 output = net(data)\n",
    "#                 loss = softmax_cross_entropy(output, label)\n",
    "#             loss.backward()\n",
    "#             trainer.step(batch_size)\n",
    "            \n",
    "#             # End of training loop for this epoch\n",
    "#             toc = time.time()\n",
    "#             train_time = toc - tic\n",
    "\n",
    "#             train_loss += nd.mean(loss).asscalar()\n",
    "#             train_acc += utilstmp.accuracy(output, label)\n",
    "\n",
    "#         test_acc = utilstmp.evaluate_accuracy(test_data, net)\n",
    "#         print(\"Epoch %d. Training Time: %.3fs, Loss: %f, Train acc %f, Test acc %f\" % (\n",
    "#             epoch, train_time, train_loss/len(train_data), train_acc/len(train_data), test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mxnet import ndarray as nd\n",
    "\n",
    "num_inputs = 56\n",
    "num_outputs = 2\n",
    "\n",
    "num_hidden = 256\n",
    "weight_scale = .01\n",
    "\n",
    "W1 = nd.random_normal(shape=(num_inputs, num_hidden), scale=weight_scale)\n",
    "b1 = nd.zeros(num_hidden)\n",
    "\n",
    "W2 = nd.random_normal(shape=(num_hidden, num_outputs), scale=weight_scale)\n",
    "b2 = nd.zeros(num_outputs)\n",
    "\n",
    "params = [W1, b1, W2, b2]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "#     X = X.reshape((-1, num_inputs))# flatten 矩阵变数组\n",
    "    h1 = relu(nd.dot(X, W1) + b1)\n",
    "    output = nd.dot(h1, W2) + b2\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-bca68c232320>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-18-bca68c232320>\u001b[0m(14)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     12 \u001b[0;31m            \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     13 \u001b[0;31m        \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 14 \u001b[0;31m        \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     16 \u001b[0;31m        \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%pdb on\n",
    "from mxnet import autograd as autograd\n",
    "\n",
    "learning_rate = .5\n",
    "\n",
    "for epoch in range(5):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    for data, label in train_data:\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        utilstmp.SGD(params, learning_rate/batch_size)\n",
    "\n",
    "        train_loss += nd.mean(loss).asscalar()\n",
    "        train_acc += utils.accuracy(output, label)\n",
    "\n",
    "    test_acc = utilstmp.evaluate_accuracy(test_data, net)\n",
    "    print(\"Epoch %d. Loss: %f, Train acc %f, Test acc %f\" % (\n",
    "        epoch, train_loss/len(train_data),\n",
    "        train_acc/len(train_data), test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3742\n",
      "[  44  423    2   44  390    2    4  493 1485    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0] 0\n"
     ]
    }
   ],
   "source": [
    "idx=0\n",
    "for l in x_train:\n",
    "    if(l[0]==44 and l[1]==423 and l[2]==2):\n",
    "        print(idx)\n",
    "        break\n",
    "    idx += 1\n",
    "print(x_train[idx], y_train[idx]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "import utilstmp\n",
    "import time\n",
    "\n",
    "def net_structure(num_hidden, num_outputs, ctx, num_embed):\n",
    "    net = gluon.nn.Sequential()\n",
    "    num_embed = num_embed # dimensions to embed words into\n",
    "    filter_size = 3\n",
    "    \n",
    "    with net.name_scope():\n",
    "        net.add(\n",
    "            nn.Embedding(vocab_size, num_embed),\n",
    "            nn.Conv1D(num_embed, 3),\n",
    "            nn.GlobalMaxPool1D(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Dense(num_outputs)\n",
    "        )\n",
    "        print(net)\n",
    "        print(net.collect_params())\n",
    "    net.initialize(ctx=ctx)\n",
    "    return net\n",
    "\n",
    "def mlp(optimizer='sgd', num_outputs=2, num_hidden=256, weight_scale=.01, learning_rate=0.0005, \n",
    "        num_epoch=10, batch_size=50, num_embed=300):\n",
    "    batch_size = batch_size\n",
    "\n",
    "    ctx = utilstmp.try_gpu(device_id=1)\n",
    "    net = net_structure(num_hidden=num_hidden, num_outputs=num_outputs, ctx=ctx, num_embed=num_embed)\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), optimizer, {'learning_rate': learning_rate})\n",
    "    utilstmp.train(train_data, test_data, net, loss, trainer, ctx, num_epochs=num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Embedding(18766 -> 128, float32)\n",
      "  (1): Conv1D(None -> 128, kernel_size=(3,), stride=(1,))\n",
      "  (2): GlobalMaxPool1D(size=(1,), stride=(1,), padding=(0,), ceil_mode=True)\n",
      "  (3): Dropout(p = 0.5)\n",
      "  (4): Dense(None -> 2, linear)\n",
      ")\n",
      "sequential1_ (\n",
      "  Parameter sequential1_embedding0_weight (shape=(18766, 128), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_conv0_weight (shape=(128, 0, 3), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_conv0_bias (shape=(128,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_dense0_weight (shape=(2, 0), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_dense0_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
      ")\n",
      "Start training on  gpu(1)\n",
      "Epoch 0. Loss: 0.694, Train acc 0.51, Test acc 0.52, Time 1.3 sec\n",
      "Epoch 1. Loss: 0.529, Train acc 0.73, Test acc 0.75, Time 1.1 sec\n",
      "Epoch 2. Loss: 0.195, Train acc 0.92, Test acc 0.75, Time 1.1 sec\n",
      "Epoch 3. Loss: 0.037, Train acc 0.99, Test acc 0.77, Time 1.1 sec\n",
      "Epoch 4. Loss: 0.006, Train acc 1.00, Test acc 0.77, Time 1.1 sec\n",
      "Epoch 5. Loss: 0.001, Train acc 1.00, Test acc 0.77, Time 1.1 sec\n",
      "Epoch 6. Loss: 0.001, Train acc 1.00, Test acc 0.77, Time 1.1 sec\n",
      "Epoch 7. Loss: 0.001, Train acc 1.00, Test acc 0.77, Time 1.1 sec\n",
      "Epoch 8. Loss: 0.000, Train acc 1.00, Test acc 0.77, Time 1.1 sec\n",
      "Epoch 9. Loss: 0.000, Train acc 1.00, Test acc 0.77, Time 1.1 sec\n"
     ]
    }
   ],
   "source": [
    "  \"\"\"\n",
    "    embedding_dim = 128  # embedding vector size\n",
    "    seq_length = 50  # maximum length of sequence\n",
    "    vocab_size = 8000  # most common words\n",
    "\n",
    "    num_filters = 100  # number of the convolution filters (feature maps)\n",
    "    kernel_sizes = [3, 4, 5]  # three kinds of kernels (windows)\n",
    "\n",
    "    dropout_prob = 0.5  # dropout rate\n",
    "    learning_rate = 1e-3  # learning rate\n",
    "    batch_size = 50  # batch size for training\n",
    "    num_epochs = 10  # total number of epochs\n",
    "\n",
    "    num_classes = 2  # number of classes\n",
    "\n",
    "    test_split = 0.1  # percentage of test data\n",
    "    \"\"\"\n",
    "mlp(learning_rate=0.003, num_epoch=10, batch_size=5, optimizer='adam', num_embed=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "import utilstmp\n",
    "import time\n",
    "\n",
    "def net_structure(num_hidden, num_outputs, ctx, num_embed):\n",
    "#     net = gluon.nn.Sequential()\n",
    "#     num_embed = num_embed # dimensions to embed words into\n",
    "#     filter_size = 3\n",
    "    \n",
    "#     with net.name_scope():\n",
    "#         net.add(\n",
    "#             nn.Embedding(vocab_size, num_embed),\n",
    "#             nn.Conv1D(num_embed, 3),\n",
    "#             nn.GlobalMaxPool1D(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Dense(num_outputs)\n",
    "#         )\n",
    "#         print(net)\n",
    "#         print(net.collect_params())\n",
    "#     net.initialize(ctx=ctx)\n",
    "\n",
    "    config = TCNNConfig()\n",
    "    config.vocab_size = vocab_size\n",
    "    V = config.vocab_size\n",
    "    E = config.embedding_dim\n",
    "    Nf = config.num_filters\n",
    "    Ks = config.kernel_sizes\n",
    "    C = config.num_classes\n",
    "    Dr = config.dropout_prob\n",
    "    net = nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(\n",
    "            nn.Embedding(V, E),\n",
    "            nn.Conv1D(Nf, Ks[0]),\n",
    "            nn.GlobalMaxPool1D(),\n",
    "            nn.Dropout(Dr),\n",
    "            nn.Dense(C)\n",
    "        )\n",
    "    net.initialize(ctx=ctx)\n",
    "    return net\n",
    "\n",
    "def mlp(optimizer='sgd', num_outputs=2, num_hidden=256, weight_scale=.01, learning_rate=0.0005, \n",
    "        num_epoch=10, batch_size=50, num_embed=300):\n",
    "    batch_size = batch_size\n",
    "\n",
    "    ctx = utilstmp.try_gpu(device_id=1)\n",
    "    net = net_structure(num_hidden=num_hidden, num_outputs=num_outputs, ctx=ctx, num_embed=num_embed)\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), optimizer, {'learning_rate': learning_rate})\n",
    "    utilstmp.train(train_data, test_data, net, loss, trainer, ctx, num_epochs=num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training on  gpu(1)\n",
      "Epoch 0. Loss: 0.693, Train acc 0.50, Test acc 0.48, Time 1.3 sec\n",
      "Epoch 1. Loss: 0.688, Train acc 0.55, Test acc 0.57, Time 1.1 sec\n",
      "Epoch 2. Loss: 0.578, Train acc 0.72, Test acc 0.72, Time 1.1 sec\n",
      "Epoch 3. Loss: 0.315, Train acc 0.88, Test acc 0.72, Time 1.1 sec\n",
      "Epoch 4. Loss: 0.134, Train acc 0.96, Test acc 0.73, Time 1.1 sec\n",
      "Epoch 5. Loss: 0.053, Train acc 0.99, Test acc 0.73, Time 1.1 sec\n",
      "Epoch 6. Loss: 0.024, Train acc 1.00, Test acc 0.73, Time 1.1 sec\n",
      "Epoch 7. Loss: 0.012, Train acc 1.00, Test acc 0.75, Time 1.1 sec\n",
      "Epoch 8. Loss: 0.007, Train acc 1.00, Test acc 0.73, Time 1.1 sec\n",
      "Epoch 9. Loss: 0.004, Train acc 1.00, Test acc 0.74, Time 1.1 sec\n"
     ]
    }
   ],
   "source": [
    "mlp(learning_rate=1e-3, num_epoch=10, batch_size=50, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-8852729f0f48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'shape'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-46-8852729f0f48>\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> 1 \u001b[0;31m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-17bdb091efb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-60-17bdb091efb0>\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> 1 \u001b[0;31m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      "[[[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   ..., \n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   ..., \n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   ..., \n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   ..., \n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   ..., \n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   ..., \n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "   [ 0.  0.  0. ...,  0.  0.  0.]]]]\n",
      "<NDArray 50x1x28x28 @cpu(0)>, \n",
      "[ 2.  2.  3.  2.  0.  7.  5.  7.  9.  0.  1.  7.  0.  2.  8.  9.  3.  3.\n",
      "  3.  7.  0.  5.  3.  1.  3.  6.  2.  4.  6.  9.  9.  1.  5.  5.  8.  7.\n",
      "  4.  6.  7.  6.  3.  3.  8.  9.  2.  6.  4.  7.  0.  7.]\n",
      "<NDArray 50 @cpu(0)>)\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize weight on gpu(0)\n",
      "Start training on  gpu(0)\n",
      "Epoch 0. Loss: 1.238, Train acc 0.53, Test acc 0.75, Time 2.2 sec\n",
      "Epoch 1. Loss: 0.535, Train acc 0.79, Test acc 0.79, Time 1.8 sec\n",
      "Epoch 2. Loss: 0.440, Train acc 0.83, Test acc 0.84, Time 1.8 sec\n",
      "Epoch 3. Loss: 0.392, Train acc 0.85, Test acc 0.87, Time 1.8 sec\n",
      "Epoch 4. Loss: 0.354, Train acc 0.87, Test acc 0.88, Time 1.8 sec\n"
     ]
    }
   ],
   "source": [
    "from mxnet.gluon import nn\n",
    "\n",
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(\n",
    "        nn.Conv2D(channels=20, kernel_size=5, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(channels=50, kernel_size=3, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Flatten(),\n",
    "        nn.Dense(128, activation=\"relu\"),\n",
    "        nn.Dense(10)\n",
    "    )\n",
    "from mxnet import gluon\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utilstmp\n",
    "\n",
    "# 初始化\n",
    "ctx = utilstmp.try_gpu()\n",
    "net.initialize(ctx=ctx)\n",
    "print('initialize weight on', ctx)\n",
    "\n",
    "# 获取数据\n",
    "batch_size = 256\n",
    "train_data, test_data = utilstmp.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "# 训练\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(),\n",
    "                        'sgd', {'learning_rate': 0.5})\n",
    "utilstmp.train(train_data, test_data, net, loss,\n",
    "            trainer, ctx, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilstmp.try_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0 0 0 ..., 0 0 0]\n",
       " [0 0 0 ..., 0 0 0]\n",
       " [0 0 0 ..., 0 0 0]\n",
       " ..., \n",
       " [0 0 0 ..., 0 0 0]\n",
       " [0 0 0 ..., 0 0 0]\n",
       " [0 0 0 ..., 0 0 0]]\n",
       "<NDArray 56x256 @cpu(0)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[   1  175    7 ...,    0    0    0]\n",
      " [  41   37    6 ...,    0    0    0]\n",
      " [  27    5    1 ...,    0    0    0]\n",
      " ..., \n",
      " [5601 3371    4 ...,    0    0    0]\n",
      " [1194   16 1140 ...,    0    0    0]\n",
      " [2690    2 5702 ...,    0    0    0]]\n",
      "<NDArray 50x56 @cpu(0)> \n",
      "[0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0\n",
      " 0 1 0 1 0 1 1 1 0 1 1 0 1]\n",
      "<NDArray 50 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "tmp=0\n",
    "for data, label in test_data:\n",
    "    print(data, label)\n",
    "    tmp=data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ -53.59818268  -66.17845154 -137.74136353 ...,  140.63609314\n",
       "    73.84160614   53.23345184]\n",
       " [ 130.25120544 -148.00364685  210.47557068 ...,    1.83334064\n",
       "    80.29856873 -223.39505005]\n",
       " [  -9.79635906   -2.76786876  -16.18431473 ...,   16.98617172   14.4361124\n",
       "   -12.94740009]\n",
       " ..., \n",
       " [ 311.0447998   -86.59976196   36.55081558 ...,  315.77960205\n",
       "  -151.91706848 -549.48803711]\n",
       " [  88.37377167   97.152771    -61.81256866 ...,   -2.53061891\n",
       "   130.60990906  -47.19386673]\n",
       " [ 145.34436035  397.92401123   14.76320648 ...,  156.37654114\n",
       "   336.85375977 -424.31954956]]\n",
       "<NDArray 50x256 @cpu(0)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.dot(tmp.astype('float32'), W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "import utilstmp\n",
    "import time\n",
    "\n",
    "def net_structure(num_hidden, num_outputs, ctx):\n",
    "    net = gluon.nn.Sequential()\n",
    "    num_embed = 300 # dimensions to embed words into\n",
    "    filter_size = 3\n",
    "    \n",
    "    with net.name_scope():\n",
    "        net.add(\n",
    "            nn.Conv2D(channels=20, kernel_size=5, activation='relu'),\n",
    "            nn.MaxPool2D(pool_size=2, strides=2),\n",
    "            nn.Conv2D(channels=50, kernel_size=3, activation='relu'),\n",
    "            nn.MaxPool2D(pool_size=2, strides=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Dense(128, activation=\"relu\"),\n",
    "            nn.Dense(10)\n",
    "        )\n",
    "        print(net)\n",
    "        print(net.collect_params())\n",
    "    \n",
    "    net.initialize(ctx=ctx)\n",
    "    return net\n",
    "\n",
    "def mlp(optimizer='sgd', num_outputs=2, num_hidden=256, weight_scale=.01, learning_rate=0.0005, \n",
    "        num_epoch=50, batch_size=50):\n",
    "    batch_size = batch_size\n",
    "#     train_data, test_data = utils.load_data_fashion_mnist(batch_size)\n",
    "    ctx = utilstmp.try_gpu()\n",
    "    net = net_structure(num_hidden=num_hidden, num_outputs=num_outputs, ctx=ctx)\n",
    "    \n",
    "#     softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    batch_size = 256\n",
    "    train_data, test_data = utilstmp.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "    # 训练\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = gluon.Trainer(net.collect_params(),\n",
    "                            'sgd', {'learning_rate': 0.5})\n",
    "    utilstmp.train(train_data, test_data, net, loss,\n",
    "                trainer, ctx, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1789,\n",
       " 44,\n",
       " 56,\n",
       " 5,\n",
       " 54,\n",
       " 451,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 1789.    44.    56.     5.    54.   451.     0.     0.     0.     0.\n",
       "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
       "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
       "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
       "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
       "     0.     0.     0.     0.     0.     0.]\n",
       "<NDArray 56 @cpu(0)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
