{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Dev split: 9662/1000\n",
      "train shape: (9662, 56)\n",
      "dev shape: (1000, 56)\n",
      "vocab_size 18766\n",
      "sentence max words 56\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "try:\n",
    "    # For Python 3.0 and later\n",
    "    from urllib.request import urlopen\n",
    "except ImportError:\n",
    "    # Fall back to Python 2's urllib2\n",
    "    from urllib2 import urlopen\n",
    "    \n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    return string.strip().lower()\n",
    "\n",
    "def download_sentences(url):\n",
    "    \"\"\"\n",
    "    Download sentences from specified URL. \n",
    "    \n",
    "    Strip trailing newline, convert to Unicode.\n",
    "    \"\"\"\n",
    "    \n",
    "    remote_file = urlopen(url)\n",
    "    return [line.decode('Latin1').strip() for line in remote_file.readlines()]\n",
    "    \n",
    "def load_data_and_labels():\n",
    "    \"\"\"\n",
    "    Loads polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    positive_examples = download_sentences('https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.pos')\n",
    "    negative_examples = download_sentences('https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.neg')\n",
    "    \n",
    "    # Tokenize\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_str(sent).split(\" \") for sent in x_text]\n",
    "\n",
    "    # Generate labels\n",
    "    positive_labels = [1 for _ in positive_examples]\n",
    "    negative_labels = [0 for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return x_text, y\n",
    "\n",
    "\n",
    "def pad_sentences(sentences, padding_word=\"\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to be the length of the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "        \n",
    "    return padded_sentences\n",
    "\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from token to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    \n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    \n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    \n",
    "    return vocabulary, vocabulary_inv\n",
    "\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentences and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = np.array([\n",
    "            [vocabulary[word] for word in sentence]\n",
    "            for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\"\"\"\n",
    "Loads and preprocesses data for the MR dataset.\n",
    "Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "\"\"\"\n",
    "# Load and preprocess data\n",
    "sentences, labels = load_data_and_labels()\n",
    "sentences_padded = pad_sentences(sentences)\n",
    "vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# split train/dev set\n",
    "# there are a total of 10662 labeled examples to train on\n",
    "x_train, x_dev = x_shuffled[:-1000], x_shuffled[-1000:]\n",
    "y_train, y_dev = y_shuffled[:-1000], y_shuffled[-1000:]\n",
    "\n",
    "sentence_size = x_train.shape[1]\n",
    "\n",
    "print('Train/Dev split: %d/%d' % (len(y_train), len(y_dev)))\n",
    "print('train shape:', x_train.shape)\n",
    "print('dev shape:', x_dev.shape)\n",
    "print('vocab_size', vocab_size)\n",
    "print('sentence max words', sentence_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1789,   44,   56, ...,    0,    0,    0],\n",
       "       [  33,  822,  174, ...,    0,    0,    0],\n",
       "       [  17,    7, 1491, ...,    0,    0,    0],\n",
       "       ..., \n",
       "       [ 114,    9,  340, ...,    0,    0,    0],\n",
       "       [  12,   94, 2965, ...,    0,    0,    0],\n",
       "       [  20, 1416,    2, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.reshape(9662, 1, 56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "batch_size = 50\n",
    "dataset_train = gluon.data.ArrayDataset(x_train.astype('float32'), y_train.astype('float32'))\n",
    "train_data = gluon.data.DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "dataset_dev = gluon.data.ArrayDataset(x_dev.astype('float32'), y_dev.astype('float32'))\n",
    "test_data = gluon.data.DataLoader(dataset_dev, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "batch_size = 50\n",
    "dataset_train = gluon.data.ArrayDataset(x_train.reshape(x_train.shape[0], 1, x_train.shape[1]).astype('float32'), y_train.astype('float32'))\n",
    "train_data = gluon.data.DataLoader(dataset_train, batch_size, shuffle=True)\n",
    "dataset_dev = gluon.data.ArrayDataset(x_dev.reshape(x_dev.shape[0], 1, x_dev.shape[1]).astype('float32'), y_dev.astype('float32'))\n",
    "test_data = gluon.data.DataLoader(dataset_dev, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[[  7.85000000e+02   7.00000000e+00   4.50000000e+01 ...,   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  4.10000000e+01   4.90000000e+01   9.33000000e+02 ...,   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  4.09700000e+03   8.00000000e+00   8.90000000e+01 ...,   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " ..., \n",
      " [[  3.00000000e+00   7.80000000e+01   2.73000000e+02 ...,   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  1.16500000e+03   2.00000000e+00   2.71700000e+03 ...,   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[  1.20000000e+01   3.00000000e+00   4.72500000e+03 ...,   0.00000000e+00\n",
      "     0.00000000e+00   0.00000000e+00]]]\n",
      "<NDArray 50x1x56 @cpu(0)> \n",
      "[ 0.  1.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0.  1.  1.  0.  0.  1.  0.\n",
      "  1.  1.  1.  0.  1.  1.  0.  1.  0.  1.  0.  1.  0.  0.  0.  0.  1.  1.\n",
      "  1.  1.  1.  0.  0.  0.  1.  1.  1.  1.  0.  1.  1.  1.]\n",
      "<NDArray 50 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "for data, label in test_data:\n",
    "    print(data, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "import utilstmp\n",
    "import time\n",
    "\n",
    "def net_structure(num_hidden, num_outputs, ctx):\n",
    "    net = gluon.nn.Sequential()\n",
    "    num_embed = 300 # dimensions to embed words into\n",
    "    filter_size = 3\n",
    "    \n",
    "    with net.name_scope():\n",
    "#         net.add(\n",
    "#             nn.Embedding(vocab_size, num_embed),\n",
    "#             nn.Conv1D(num_embed, 3),\n",
    "#             nn.GlobalMaxPool1D(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Dense(num_outputs)\n",
    "#         )\n",
    "#         net.add(gluon.nn.Flatten())\n",
    "        net.add(gluon.nn.Embedding(input_dim=vocab_size, output_dim=num_embed))\n",
    "        # 输入输出数据格式是 batch x channel x height x width\n",
    "        net.add(gluon.nn.Conv1D(channels=1, kernel_size=filter_size, activation='relu'))\n",
    "        net.add(gluon.nn.GlobalMaxPool1D())\n",
    "        net.add(gluon.nn.Dropout(0.5))\n",
    "        net.add(gluon.nn.Dense(num_outputs, activation=\"relu\"))\n",
    "#         net.add(gluon.nn.softmax_cross_entropy)\n",
    "        print(net)\n",
    "        print(net.collect_params())\n",
    "    \n",
    "    net.initialize(ctx=ctx)\n",
    "    return net\n",
    "\n",
    "def mlp(optimizer='sgd', num_outputs=2, num_hidden=256, weight_scale=.01, learning_rate=0.0005, \n",
    "        num_epoch=50, batch_size=50):\n",
    "    batch_size = batch_size\n",
    "#     train_data, test_data = utils.load_data_fashion_mnist(batch_size)\n",
    "    ctx = utilstmp.try_gpu()\n",
    "#     net = net_structure(num_hidden=num_hidden, num_outputs=num_outputs, ctx=ctx)\n",
    "#     softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': config.learning_rate})\n",
    "#     import pdb\n",
    "#     pdb.set_trace() \n",
    "    utilstmp.train(train_data, test_data, net, loss, trainer, ctx, num_epochs=num_epoch)\n",
    "#     for epoch in range(num_epoch):\n",
    "#         tic = time.time()\n",
    "#         train_loss = 0.\n",
    "#         train_acc = 0.\n",
    "#         for data, label in train_data:\n",
    "#             with autograd.record():\n",
    "#                 output = net(data)\n",
    "#                 loss = softmax_cross_entropy(output, label)\n",
    "#             loss.backward()\n",
    "#             trainer.step(batch_size)\n",
    "            \n",
    "#             # End of training loop for this epoch\n",
    "#             toc = time.time()\n",
    "#             train_time = toc - tic\n",
    "\n",
    "#             train_loss += nd.mean(loss).asscalar()\n",
    "#             train_acc += utilstmp.accuracy(output, label)\n",
    "\n",
    "#         test_acc = utilstmp.evaluate_accuracy(test_data, net)\n",
    "#         print(\"Epoch %d. Training Time: %.3fs, Loss: %f, Train acc %f, Test acc %f\" % (\n",
    "#             epoch, train_time, train_loss/len(train_data), train_acc/len(train_data), test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = batch_size\n",
    "#     train_data, test_data = utils.load_data_fashion_mnist(batch_size)\n",
    "ctx = utilstmp.try_gpu()\n",
    "#     net = net_structure(num_hidden=num_hidden, num_outputs=num_outputs, ctx=ctx)\n",
    "net = gluon.nn.Sequential()\n",
    "num_embed = 300 # dimensions to embed words into\n",
    "filter_size = 3\n",
    "\n",
    "with net.name_scope():\n",
    "#     net.add(\n",
    "#         nn.Embedding(vocab_size, num_embed),\n",
    "#         nn.Conv1D(num_embed, 3),\n",
    "#         nn.GlobalMaxPool1D(),\n",
    "#         nn.Dropout(0.5),\n",
    "#         nn.Dense(2)\n",
    "#     )\n",
    "    net.add(gluon.nn.Embedding(input_dim=vocab_size, output_dim=num_embed))\n",
    "    # 输入输出数据格式是 batch x channel x height x width\n",
    "    net.add(gluon.nn.Conv1D(channels=1, kernel_size=filter_size, activation='relu'))\n",
    "    net.add(gluon.nn.GlobalMaxPool1D())\n",
    "    net.add(gluon.nn.Dropout(0.5))\n",
    "    net.add(gluon.nn.Dense(num_outputs, activation=\"relu\"))\n",
    "net.initialize(ctx=ctx)\n",
    "#     softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = utilstmp.try_gpu()\n",
    "config = TCNNConfig()\n",
    "config.vocab_size = vocab_size\n",
    "V = config.vocab_size\n",
    "E = config.embedding_dim\n",
    "Nf = config.num_filters\n",
    "Ks = config.kernel_sizes\n",
    "C = config.num_classes\n",
    "Dr = config.dropout_prob\n",
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(\n",
    "        nn.Embedding(V, E),\n",
    "        nn.Conv1D(Nf, Ks[0]),\n",
    "        nn.GlobalMaxPool1D(),\n",
    "        nn.Dropout(Dr),\n",
    "        nn.Dense(C)\n",
    "    )\n",
    "net.initialize(ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training on  gpu(0)\n",
      "Epoch 0. Loss: 0.694, Train acc 0.50, Test acc 0.48, Time 1.3 sec\n",
      "Epoch 1. Loss: 0.683, Train acc 0.57, Test acc 0.60, Time 1.1 sec\n",
      "Epoch 2. Loss: 0.518, Train acc 0.76, Test acc 0.73, Time 1.1 sec\n",
      "Epoch 3. Loss: 0.256, Train acc 0.91, Test acc 0.74, Time 1.1 sec\n",
      "Epoch 4. Loss: 0.096, Train acc 0.97, Test acc 0.73, Time 1.1 sec\n",
      "Epoch 5. Loss: 0.032, Train acc 0.99, Test acc 0.73, Time 1.1 sec\n",
      "Epoch 6. Loss: 0.013, Train acc 1.00, Test acc 0.73, Time 1.1 sec\n",
      "Epoch 7. Loss: 0.007, Train acc 1.00, Test acc 0.73, Time 1.1 sec\n",
      "Epoch 8. Loss: 0.004, Train acc 1.00, Test acc 0.73, Time 1.1 sec\n",
      "Epoch 9. Loss: 0.003, Train acc 1.00, Test acc 0.73, Time 1.1 sec\n"
     ]
    }
   ],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': config.learning_rate})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "utilstmp.train(train_data, test_data, net, loss, trainer, ctx, num_epochs=config.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training on  gpu(0)\n",
      "Epoch 0. Loss: 0.694, Train acc 0.49, Test acc 0.51, Time 1.1 sec\n",
      "Epoch 1. Loss: 0.693, Train acc 0.50, Test acc 0.50, Time 1.0 sec\n",
      "Epoch 2. Loss: 0.693, Train acc 0.50, Test acc 0.50, Time 1.0 sec\n",
      "Epoch 3. Loss: 0.693, Train acc 0.50, Test acc 0.49, Time 1.0 sec\n",
      "Epoch 4. Loss: 0.693, Train acc 0.50, Test acc 0.50, Time 1.1 sec\n"
     ]
    }
   ],
   "source": [
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': config.learning_rate})\n",
    "#     import pdb\n",
    "#     pdb.set_trace() \n",
    "utilstmp.train(train_data, test_data, net, loss, trainer, ctx, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training on  gpu(0)\n",
      "Epoch 0. Loss: 0.631, Train acc 0.57, Test acc 0.55, Time 1.5 sec\n",
      "Epoch 1. Loss: 0.607, Train acc 0.62, Test acc 0.56, Time 1.2 sec\n",
      "Epoch 2. Loss: 0.588, Train acc 0.64, Test acc 0.57, Time 1.2 sec\n",
      "Epoch 3. Loss: 0.573, Train acc 0.67, Test acc 0.57, Time 1.2 sec\n",
      "Epoch 4. Loss: 0.561, Train acc 0.68, Test acc 0.59, Time 1.2 sec\n",
      "Epoch 5. Loss: 0.548, Train acc 0.70, Test acc 0.58, Time 1.2 sec\n",
      "Epoch 6. Loss: 0.543, Train acc 0.71, Test acc 0.60, Time 1.2 sec\n",
      "Epoch 7. Loss: 0.534, Train acc 0.72, Test acc 0.59, Time 1.3 sec\n",
      "Epoch 8. Loss: 0.534, Train acc 0.73, Test acc 0.61, Time 1.2 sec\n",
      "Epoch 9. Loss: 0.530, Train acc 0.73, Test acc 0.61, Time 1.2 sec\n",
      "Epoch 10. Loss: 0.533, Train acc 0.73, Test acc 0.59, Time 1.2 sec\n",
      "Epoch 11. Loss: 0.526, Train acc 0.74, Test acc 0.60, Time 1.2 sec\n",
      "Epoch 12. Loss: 0.532, Train acc 0.73, Test acc 0.59, Time 1.3 sec\n",
      "Epoch 13. Loss: 0.522, Train acc 0.74, Test acc 0.59, Time 1.2 sec\n",
      "Epoch 14. Loss: 0.521, Train acc 0.74, Test acc 0.59, Time 1.2 sec\n",
      "Epoch 15. Loss: 0.525, Train acc 0.74, Test acc 0.59, Time 1.2 sec\n",
      "Epoch 16. Loss: 0.523, Train acc 0.74, Test acc 0.60, Time 1.3 sec\n",
      "Epoch 17. Loss: 0.522, Train acc 0.74, Test acc 0.59, Time 1.2 sec\n",
      "Epoch 18. Loss: 0.527, Train acc 0.74, Test acc 0.60, Time 1.2 sec\n",
      "Epoch 19. Loss: 0.523, Train acc 0.74, Test acc 0.61, Time 1.2 sec\n",
      "Epoch 20. Loss: 0.523, Train acc 0.74, Test acc 0.61, Time 1.2 sec\n",
      "Epoch 21. Loss: 0.527, Train acc 0.74, Test acc 0.60, Time 1.3 sec\n",
      "Epoch 22. Loss: 0.520, Train acc 0.75, Test acc 0.60, Time 1.2 sec\n",
      "Epoch 23. Loss: 0.520, Train acc 0.75, Test acc 0.60, Time 1.2 sec\n",
      "Epoch 24. Loss: 0.523, Train acc 0.74, Test acc 0.60, Time 1.2 sec\n",
      "Epoch 25. Loss: 0.524, Train acc 0.74, Test acc 0.61, Time 1.3 sec\n",
      "Epoch 26. Loss: 0.520, Train acc 0.75, Test acc 0.60, Time 1.2 sec\n",
      "Epoch 27. Loss: 0.521, Train acc 0.75, Test acc 0.61, Time 1.2 sec\n",
      "Epoch 28. Loss: 0.522, Train acc 0.74, Test acc 0.60, Time 1.2 sec\n",
      "Epoch 29. Loss: 0.523, Train acc 0.74, Test acc 0.60, Time 1.2 sec\n",
      "Epoch 30. Loss: 0.516, Train acc 0.75, Test acc 0.60, Time 1.2 sec\n",
      "Epoch 31. Loss: 0.524, Train acc 0.74, Test acc 0.61, Time 1.2 sec\n",
      "Epoch 32. Loss: 0.522, Train acc 0.75, Test acc 0.60, Time 1.2 sec\n",
      "Epoch 33. Loss: 0.521, Train acc 0.75, Test acc 0.60, Time 1.2 sec\n",
      "Epoch 34. Loss: 0.521, Train acc 0.75, Test acc 0.60, Time 1.3 sec\n",
      "Epoch 35. Loss: 0.525, Train acc 0.74, Test acc 0.60, Time 1.2 sec\n",
      "Epoch 36. Loss: 0.518, Train acc 0.75, Test acc 0.60, Time 1.2 sec\n",
      "Epoch 37. Loss: 0.523, Train acc 0.74, Test acc 0.60, Time 1.2 sec\n",
      "Epoch 38. Loss: 0.523, Train acc 0.74, Test acc 0.60, Time 1.2 sec\n",
      "Epoch 39. Loss: 0.521, Train acc 0.75, Test acc 0.60, Time 1.2 sec\n",
      "Epoch 40. Loss: 0.522, Train acc 0.75, Test acc 0.60, Time 1.2 sec\n",
      "Epoch 41. Loss: 0.525, Train acc 0.74, Test acc 0.60, Time 1.2 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-f97f11ef4770>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# %pdb on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-122-a2479eb5bc44>\u001b[0m in \u001b[0;36mmlp\u001b[0;34m(optimizer, num_outputs, num_hidden, weight_scale, learning_rate, num_epoch, batch_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#     import pdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m#     pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mutilstmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;31m#     for epoch in range(num_epoch):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#         tic = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/cnn/utilstmp.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data, test_data, net, loss, trainer, ctx, num_epochs, print_batches)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/cnn/utilstmp.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/block.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;34m\"\"\"Calls forward. Only accepts positional arguments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/nn/basic_layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/block.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;34m\"\"\"Calls forward. Only accepts positional arguments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/block.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m    573\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_cached_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reg_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhybrid_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/nn/basic_layers.py\u001b[0m in \u001b[0;36mhybrid_forward\u001b[0;34m(self, F, x, weight, bias)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhybrid_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         act = F.FullyConnected(x, weight, bias, no_bias=bias is None, num_hidden=self._units,\n\u001b[0;32m--> 206\u001b[0;31m                                flatten=self._flatten, name='fwd')\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/ndarray/register.py\u001b[0m in \u001b[0;36mFullyConnected\u001b[0;34m(data, weight, bias, num_hidden, no_bias, flatten, out, name, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/_ctypes/ndarray.py\u001b[0m in \u001b[0;36m_imperative_invoke\u001b[0;34m(handle, ndargs, keys, vals, out)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         ctypes.byref(out_stypes)))\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moriginal_output\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %pdb on\n",
    "mlp(num_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb off\n",
    "class TCNNConfig(object):\n",
    "    \"\"\"\n",
    "    CNN parameters\n",
    "    \"\"\"\n",
    "    embedding_dim = 128  # embedding vector size\n",
    "    seq_length = 50  # maximum length of sequence\n",
    "    vocab_size = 8000  # most common words\n",
    "\n",
    "    num_filters = 100  # number of the convolution filters (feature maps)\n",
    "    kernel_sizes = [3, 4, 5]  # three kinds of kernels (windows)\n",
    "\n",
    "    dropout_prob = 0.5  # dropout rate\n",
    "    learning_rate = 1e-3  # learning rate\n",
    "    batch_size = 50  # batch size for training\n",
    "    num_epochs = 10  # total number of epochs\n",
    "\n",
    "    num_classes = 2  # number of classes\n",
    "\n",
    "    dev_split = 0.1  # percentage of dev data\n",
    "\n",
    "\n",
    "class Conv_Max_Pooling(nn.Block):\n",
    "    \"\"\"\n",
    "    Integration of Conv1D and GlobalMaxPool1D layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, kernel_size, **kwargs):\n",
    "        super(Conv_Max_Pooling, self).__init__(**kwargs)\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.conv = nn.Conv1D(channels, kernel_size)\n",
    "            self.pooling = nn.GlobalMaxPool1D()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pooling(self.conv(x))\n",
    "        return nd.relu(output).flatten()\n",
    "\n",
    "\n",
    "class TextCNN(nn.Block):\n",
    "    \"\"\"\n",
    "    CNN text classification model, based on the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super(TextCNN, self).__init__(**kwargs)\n",
    "\n",
    "        V = config.vocab_size\n",
    "        E = config.embedding_dim\n",
    "        Nf = config.num_filters\n",
    "        Ks = config.kernel_sizes\n",
    "        C = config.num_classes\n",
    "        Dr = config.dropout_prob\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(V, E)  # embedding layer\n",
    "\n",
    "            # three different convolutional layers\n",
    "            self.conv1 = Conv_Max_Pooling(Nf, Ks[0])\n",
    "            self.conv2 = Conv_Max_Pooling(Nf, Ks[1])\n",
    "            self.conv3 = Conv_Max_Pooling(Nf, Ks[2])\n",
    "            self.dropout = nn.Dropout(Dr)  # a dropout layer\n",
    "            self.fc1 = nn.Dense(C)  # a dense layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).transpose((0, 2, 1))  # Conv1D takes in NCW as input\n",
    "        o1, o2, o3 = self.conv1(x), self.conv2(x), self.conv3(x)\n",
    "        outputs = self.fc1(self.dropout(nd.concat(o1, o2, o3)))\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "\n",
    "ctx = utilstmp.try_gpu()\n",
    "config = TCNNConfig()\n",
    "config.vocab_size = vocab_size\n",
    "V = config.vocab_size\n",
    "E = config.embedding_dim\n",
    "Nf = config.num_filters\n",
    "Ks = config.kernel_sizes\n",
    "C = config.num_classes\n",
    "Dr = config.dropout_prob\n",
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(\n",
    "        nn.Embedding(V, E),\n",
    "        nn.Conv1D(Nf, Ks[0]),\n",
    "        nn.GlobalMaxPool1D(),\n",
    "        nn.Dropout(Dr),\n",
    "        nn.Dense(C)\n",
    "    )\n",
    "net.initialize(ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training on  gpu(0)\n",
      "Epoch 0. Loss: nan, Train acc 0.50, Test acc 0.52, Time 1.5 sec\n",
      "Epoch 1. Loss: nan, Train acc 0.50, Test acc 0.52, Time 1.4 sec\n",
      "Epoch 2. Loss: nan, Train acc 0.50, Test acc 0.52, Time 1.4 sec\n",
      "Epoch 3. Loss: nan, Train acc 0.50, Test acc 0.52, Time 1.4 sec\n",
      "Epoch 4. Loss: nan, Train acc 0.50, Test acc 0.52, Time 1.4 sec\n",
      "Epoch 5. Loss: nan, Train acc 0.50, Test acc 0.52, Time 1.4 sec\n",
      "Epoch 6. Loss: nan, Train acc 0.50, Test acc 0.52, Time 1.4 sec\n",
      "Epoch 7. Loss: nan, Train acc 0.50, Test acc 0.52, Time 1.4 sec\n",
      "Epoch 8. Loss: nan, Train acc 0.50, Test acc 0.52, Time 1.4 sec\n",
      "Epoch 9. Loss: nan, Train acc 0.50, Test acc 0.52, Time 1.4 sec\n"
     ]
    }
   ],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'adamax', {'learning_rate': config.learning_rate})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "utilstmp.train(train_data, test_data, net, loss, trainer, ctx, num_epochs=config.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mxnet import ndarray as nd\n",
    "\n",
    "num_inputs = 56\n",
    "num_outputs = 2\n",
    "\n",
    "num_hidden = 256\n",
    "weight_scale = .01\n",
    "\n",
    "W1 = nd.random_normal(shape=(num_inputs, num_hidden), scale=weight_scale)\n",
    "b1 = nd.zeros(num_hidden)\n",
    "\n",
    "W2 = nd.random_normal(shape=(num_hidden, num_outputs), scale=weight_scale)\n",
    "b2 = nd.zeros(num_outputs)\n",
    "\n",
    "params = [W1, b1, W2, b2]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "#     X = X.reshape((-1, num_inputs))# flatten 矩阵变数组\n",
    "    h1 = relu(nd.dot(X, W1) + b1)\n",
    "    output = nd.dot(h1, W2) + b2\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    },
    {
     "ename": "MXNetError",
     "evalue": "[23:57:32] src/operator/contrib/../elemwise_op_common.h:123: Check failed: assign(&dattr, (*vec)[i]) Incompatible attr in node  at 1-th input: expected int64, got float32\n\nStack trace returned 10 entries:\n[bt] (0) /home/work/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2bc5e8) [0x7fab1986a5e8]\n[bt] (1) /home/work/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2bc9f8) [0x7fab1986a9f8]\n[bt] (2) /home/work/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2e2cb8) [0x7fab19890cb8]\n[bt] (3) /home/work/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2e37c3) [0x7fab198917c3]\n[bt] (4) /home/work/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x26b14ce) [0x7fab1bc5f4ce]\n[bt] (5) /home/work/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x26b3709) [0x7fab1bc61709]\n[bt] (6) /home/work/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x25f265b) [0x7fab1bba065b]\n[bt] (7) /home/work/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(MXImperativeInvokeEx+0x63) [0x7fab1bba0bc3]\n[bt] (8) /home/work/anaconda3/envs/gluon/lib/python3.6/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x7fab65378ec0]\n[bt] (9) /home/work/anaconda3/envs/gluon/lib/python3.6/lib-dynload/../../libffi.so.6(ffi_call+0x22d) [0x7fab6537887d]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-bca68c232320>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-a0cd1617c852>\u001b[0m in \u001b[0;36mnet\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#     X = X.reshape((-1, num_inputs))# flatten 矩阵变数组\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/ndarray/register.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(lhs, rhs, transpose_a, transpose_b, out, name, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/_ctypes/ndarray.py\u001b[0m in \u001b[0;36m_imperative_invoke\u001b[0;34m(handle, ndargs, keys, vals, out)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         ctypes.byref(out_stypes)))\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moriginal_output\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \"\"\"\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMXNetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMXGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMXNetError\u001b[0m: [23:57:32] src/operator/contrib/../elemwise_op_common.h:123: Check failed: assign(&dattr, (*vec)[i]) Incompatible attr in node  at 1-th input: expected int64, got float32\n\nStack trace returned 10 entries:\n[bt] (0) /home/work/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2bc5e8) [0x7fab1986a5e8]\n[bt] (1) /home/work/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2bc9f8) [0x7fab1986a9f8]\n[bt] (2) /home/work/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2e2cb8) [0x7fab19890cb8]\n[bt] (3) /home/work/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2e37c3) [0x7fab198917c3]\n[bt] (4) /home/work/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x26b14ce) [0x7fab1bc5f4ce]\n[bt] (5) /home/work/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x26b3709) [0x7fab1bc61709]\n[bt] (6) /home/work/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x25f265b) [0x7fab1bba065b]\n[bt] (7) /home/work/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/libmxnet.so(MXImperativeInvokeEx+0x63) [0x7fab1bba0bc3]\n[bt] (8) /home/work/anaconda3/envs/gluon/lib/python3.6/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x7fab65378ec0]\n[bt] (9) /home/work/anaconda3/envs/gluon/lib/python3.6/lib-dynload/../../libffi.so.6(ffi_call+0x22d) [0x7fab6537887d]\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/work/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/base.py\u001b[0m(148)\u001b[0;36mcheck_call\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    146 \u001b[0;31m    \"\"\"\n",
      "\u001b[0m\u001b[0;32m    147 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 148 \u001b[0;31m        \u001b[0;32mraise\u001b[0m \u001b[0mMXNetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMXGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    149 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    150 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> up\n",
      "> \u001b[0;32m/home/work/anaconda3/envs/gluon/lib/python3.6/site-packages/mxnet/_ctypes/ndarray.py\u001b[0m(92)\u001b[0;36m_imperative_invoke\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     90 \u001b[0;31m        \u001b[0mc_str_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     91 \u001b[0;31m        \u001b[0mc_str_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 92 \u001b[0;31m        ctypes.byref(out_stypes)))\n",
      "\u001b[0m\u001b[0;32m     93 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     94 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0moriginal_output\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> up\n",
      "> \u001b[0;32m<string>\u001b[0m(72)\u001b[0;36mdot\u001b[0;34m()\u001b[0m\n",
      "\n",
      "ipdb> up\n",
      "> \u001b[0;32m<ipython-input-38-a0cd1617c852>\u001b[0m(3)\u001b[0;36mnet\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      1 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      2 \u001b[0;31m\u001b[0;31m#     X = X.reshape((-1, num_inputs))# flatten 矩阵变数组\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 3 \u001b[0;31m    \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      4 \u001b[0;31m    \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      5 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> X.shape\n",
      "(50, 56)\n",
      "ipdb> W1.shape\n",
      "(56, 256)\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%pdb on\n",
    "from mxnet import autograd as autograd\n",
    "\n",
    "learning_rate = .5\n",
    "\n",
    "for epoch in range(5):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    for data, label in train_data:\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        utils.SGD(params, learning_rate/batch_size)\n",
    "\n",
    "        train_loss += nd.mean(loss).asscalar()\n",
    "        train_acc += utils.accuracy(output, label)\n",
    "\n",
    "    test_acc = utils.evaluate_accuracy(test_data, net)\n",
    "    print(\"Epoch %d. Loss: %f, Train acc %f, Test acc %f\" % (\n",
    "        epoch, train_loss/len(train_data),\n",
    "        train_acc/len(train_data), test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
